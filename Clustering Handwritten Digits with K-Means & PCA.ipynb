{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a34sJZ1cPMCt"
      },
      "source": [
        "# CS 584 :: Data Mining :: George Mason University :: Spring 2024\n",
        "\n",
        "\n",
        "# Homework 3: Clustering\n",
        "\n",
        "- **100 points [8% of your final grade]**\n",
        "- **Due Sunday, March 31 by 11:59pm**\n",
        "\n",
        "- *Goals of this homework:* (1) implement your K-means model; (2) implement K-means++; (3) Apply PCA to K-Means.\n",
        "\n",
        "- *Submission instructions:* for this homework, you only need to submit to Blackboard. Please name your submission **FirstName_Lastname_hw3.ipynb**, so for example, my submission would be something like **Ziwei_Zhu_hw3.ipynb**. Your notebook should be fully executed so that we can see all outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDaoRsgmPMCu"
      },
      "source": [
        "## Part 1: K-Means (60 points)\n",
        "\n",
        "In this part, you will implement your own K-means algorithm to conduct clustering on handwritten digit images. In this homework, we will still use the handwritten digit image dataset we have already used in previous homework. However, since clustering is unsupervised learning, which means we do not use the label information anymore. So, here, we will only use the testing data stored in the \"test.txt\" file.\n",
        "\n",
        "First, let's load the data by excuting the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQr56yHnPMCv",
        "outputId": "29bb8580-3663-40e0-bab2-38821edcb487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array of testing feature matrix: shape (10000, 784)\n",
            "array of testing label matrix: shape (10000,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "test = np.loadtxt(\"test.txt\", delimiter=',')\n",
        "test_features = test[:, 1:]\n",
        "test_labels = test[:, 0]\n",
        "print('array of testing feature matrix: shape ' + str(np.shape(test_features)))\n",
        "print('array of testing label matrix: shape ' + str(np.shape(test_labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7P54xBWPMCw"
      },
      "source": [
        "Now, it's time for you to implement your own K-means algorithm. First, please write your code to build your K-means model using the image data with **K = 10**, and **Euclidean distance**.\n",
        "\n",
        "**Note: You should implement the algorithm by yourself. You are NOT allowed to use Machine Learning libraries like Sklearn**\n",
        "\n",
        "**Note: you need to decide when to stop the iteration.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RJvwZnTaPMCw"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "import random\n",
        "\n",
        "random_id_val = np.random.choice(len(test_features),10,replace=False)\n",
        "centroids = [ test_features[i] for i in random_id_val ]\n",
        "itr = 0\n",
        "old_centroids = None\n",
        "\n",
        "while np.not_equal(centroids,old_centroids).any():\n",
        "    clusters = [[] for i in range(10)]\n",
        "    for test_sample in test_features:\n",
        "        distance = np.sqrt(np.sum((test_sample - centroids)**2, axis=1))\n",
        "        clusters[np.argmin(distance)].append(test_sample)\n",
        "    old_centroids = centroids\n",
        "    centroids = [np.mean(cluster, axis=0) for cluster in clusters]\n",
        "    for i, centroid in enumerate (centroids):\n",
        "        if np.isnan(centroid).any():\n",
        "            centroids[i] = old_centroids[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCS1uF14PMCw"
      },
      "source": [
        "Next, you need to calculate the square root of Sum of Squared Error (SSE) of each cluster generated by your K-means algorithm. Then, print out the averaged SSE of your algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmW1eYxJPMCw",
        "outputId": "63f44def-2094-4b69-c439-d311e3b89b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Square root of SSE for each cluster:\n",
            "Cluster 0: 37960.512352599784\n",
            "Cluster 1: 55895.28703055209\n",
            "Cluster 2: 50984.530095863716\n",
            "Cluster 3: 59854.798019528665\n",
            "Cluster 4: 49752.04764722419\n",
            "Cluster 5: 51780.90429753062\n",
            "Cluster 6: 50890.42663484296\n",
            "Cluster 7: 54384.91897866432\n",
            "Cluster 8: 40558.438083125315\n",
            "Cluster 9: 48342.94296953176\n",
            "\n",
            "Average SSE: 50040.48061094634\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "clust_sse = []\n",
        "\n",
        "for i in range(10):\n",
        "    sse =0\n",
        "    for sample in clusters[i]:\n",
        "        sse += np.sum((sample - centroids[i])**2)\n",
        "    clust_sse.append(np.sqrt(sse))\n",
        "\n",
        "avg_sse = np.mean(clust_sse)\n",
        "print(\"Square root of SSE for each cluster:\")\n",
        "for i, sse in enumerate(clust_sse):\n",
        "  print (f\"Cluster {i:1}: {sse}\")\n",
        "\n",
        "print(f\"\\nAverage SSE: {avg_sse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MaDvwdBPMCw"
      },
      "source": [
        "Then, please have a look on https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html#sklearn.metrics.homogeneity_completeness_v_measure, and use this function to print out the homogeneity, completeness, and v-measure of your K-means model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeGJtAvPPMCw",
        "outputId": "cbd333a6-3655-4a58-b995-3ab9377497e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Homogeneity: 0.5044867648538972\n",
            "Completeness: 0.516158347413129\n",
            "V-measure: 0.510255820968763\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "from sklearn.metrics import homogeneity_completeness_v_measure\n",
        "\n",
        "# Assigning labels to clusters (labels_pred)\n",
        "labels_pred = np.zeros(len(test_labels))\n",
        "for i, cluster in enumerate(clusters):\n",
        "    for sample in cluster:\n",
        "        index = np.where((test_features == sample).all(axis=1))[0][0]\n",
        "        labels_pred[index] = i\n",
        "\n",
        "homogeneity, completeness, v_measure = homogeneity_completeness_v_measure(test_labels, labels_pred)\n",
        "\n",
        "print(f'Homogeneity: {homogeneity}')\n",
        "print(f'Completeness: {completeness}')\n",
        "print(f'V-measure: {v_measure}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I34bTet8PMCw"
      },
      "source": [
        "## Part 2: K-Means++ (30 points)\n",
        "\n",
        "Ok, now you already have a good model. But can you further improve it? In the next cell, please implement the K-means++ model introduced in the lecture.\n",
        "\n",
        "**Note: Everything else is the same as Part1, i.e., K=10, and you need to use Euclidean distance.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Wc6wLPDWPMCx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def kmeans_plus_plus_centroids(data, K=10):\n",
        "    initial_centroid_idx = np.random.choice(len(data))\n",
        "    centroids = [data[initial_centroid_idx]]\n",
        "\n",
        "    for _ in range(1, K):\n",
        "        distances = np.array([min([np.sum((x - centroid)**2) for centroid in centroids]) for x in data])\n",
        "        probabilities = distances / distances.sum()\n",
        "        cumulative_probabilities = probabilities.cumsum()\n",
        "        r = np.random.rand()\n",
        "\n",
        "        for i, p in enumerate(cumulative_probabilities):\n",
        "            if r < p:\n",
        "                centroids.append(data[i])\n",
        "                break\n",
        "\n",
        "    return centroids\n",
        "\n",
        "K = 10\n",
        "centroids = kmeans_plus_plus_centroids(test_features, K)\n",
        "\n",
        "itr = 0\n",
        "old_centroids = None\n",
        "\n",
        "while np.not_equal(centroids,old_centroids).any():\n",
        "    clusters = [[] for _ in range(K)]\n",
        "    for test_sample in test_features:\n",
        "        distance = np.sqrt(np.sum((test_sample - centroids)**2, axis=1))\n",
        "        clusters[np.argmin(distance)].append(test_sample)\n",
        "    old_centroids = centroids\n",
        "    centroids = [np.mean(cluster, axis=0) for cluster in clusters if len(cluster) > 0]\n",
        "    for i, centroid in enumerate(centroids):\n",
        "        if np.isnan(centroid).any():\n",
        "            centroids[i] = old_centroids[i]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vms7-2QEPMCx"
      },
      "source": [
        "In the next cell, use sklearn.metrics.homogeneity_completeness_v_measure() to print out the homogeneity, completeness, and v-measure of your K-means++ model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmjzHMq6PMCx",
        "outputId": "9857ca80-303b-4e9d-9e6d-49df7058733b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Homogeneity: 0.47769987588798835\n",
            "Completeness: 0.48050235939018887\n",
            "V-measure: 0.47909701938430954\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import homogeneity_completeness_v_measure\n",
        "\n",
        "labels_pred = np.zeros(len(test_features))  # Initialize array to hold predicted labels\n",
        "for cluster_id, cluster in enumerate(clusters):\n",
        "    for sample in cluster:\n",
        "        # Find the index of this sample in the original dataset\n",
        "        index = np.where((test_features == sample).all(axis=1))[0][0]\n",
        "        labels_pred[index] = cluster_id  # Assign cluster ID as the predicted label\n",
        "\n",
        "homogeneity, completeness, v_measure = homogeneity_completeness_v_measure(test_labels, labels_pred)\n",
        "\n",
        "print(f'Homogeneity: {homogeneity}')\n",
        "print(f'Completeness: {completeness}')\n",
        "print(f'V-measure: {v_measure}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lYnXvGjPMCx"
      },
      "source": [
        "## Part 3: Dimension Reduction by PCA (10 points)\n",
        "\n",
        "Last, in this part, please resue your code of PCA from HW1 to reduce the feature dimension here. And then, apply your K-Means code to the reduced data and report homogeneity, completeness, and v-measure.\n",
        "\n",
        "**Note: You need to consider the reduced dimension m of PCA as a hyper-parameter to tune. That is, you need to try different m and measure the corresponding clustering performance. At the end, you need to report the best m and clustering performance.**\n",
        "\n",
        "**Note: Everything else is the same as Part1, i.e., K=10, and you need to use Euclidean distance.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "skpjaWgGPMCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2c1446-a1f1-4add-8e1d-06c25bf44b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of transformed training data: (10000, 100)\n",
            "Square root of SSE for each cluster:\n",
            "Cluster 0: 46961.36884687147\n",
            "Cluster 1: 50277.58179942565\n",
            "Cluster 2: 49332.93775344256\n",
            "Cluster 3: 54834.137235201386\n",
            "Cluster 4: 35080.218930415154\n",
            "Cluster 5: 57992.121049033456\n",
            "Cluster 6: 32352.686638317973\n",
            "Cluster 7: 46586.92224534605\n",
            "Cluster 8: 47361.20172114954\n",
            "Cluster 9: 47834.895917437825\n",
            "\n",
            "Average SSE: 46861.40721366411\n",
            "Homogeneity: 0.5037010963293469\n",
            "Completeness: 0.5069607455373373\n",
            "V-measure: 0.5053256643220212\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "from sklearn.metrics import homogeneity_completeness_v_measure\n",
        "import numpy as np\n",
        "\n",
        "class PCA:\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.components = None\n",
        "        self.mean = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.mean = np.mean(X, axis=0)\n",
        "        X_centered = X - self.mean\n",
        "\n",
        "        _, _, v = np.linalg.svd(X_centered, full_matrices=False)\n",
        "\n",
        "        self.components = v[:self.n_components]\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_centered = X - self.mean\n",
        "        return np.dot(X_centered, self.components.T)\n",
        "\n",
        "# Instantiate PCA and fit to training data\n",
        "pca = PCA(100)\n",
        "pca.fit(test_features)\n",
        "\n",
        "# Transform the training data\n",
        "test_features_pca = pca.transform(test_features)\n",
        "print('Shape of transformed training data:', test_features_pca.shape)\n",
        "\n",
        "# K-Means algorithm code\n",
        "random_id_val = np.random.choice(len(test_features_pca), 10, replace=False)\n",
        "centroids = [test_features_pca[i] for i in random_id_val]\n",
        "itr = 0\n",
        "old_centroids = None\n",
        "\n",
        "while np.not_equal(centroids, old_centroids).any():\n",
        "    clusters = [[] for i in range(10)]\n",
        "    for test_sample in test_features_pca:\n",
        "        distance = np.sqrt(np.sum((test_sample - centroids)**2, axis=1))\n",
        "        clusters[np.argmin(distance)].append(test_sample)\n",
        "    old_centroids = centroids\n",
        "    centroids = [np.mean(cluster, axis=0) for cluster in clusters]\n",
        "    for i, centroid in enumerate(centroids):\n",
        "        if np.isnan(centroid).any():\n",
        "            centroids[i] = old_centroids[i]\n",
        "\n",
        "# Code for SSE and Average SSE\n",
        "clust_sse = []\n",
        "\n",
        "for i in range(10):\n",
        "    sse = 0\n",
        "    for sample in clusters[i]:\n",
        "        sse += np.sum((sample - centroids[i])**2)\n",
        "    clust_sse.append(np.sqrt(sse))\n",
        "\n",
        "avg_sse = np.mean(clust_sse)\n",
        "print(\"Square root of SSE for each cluster:\")\n",
        "for i, sse in enumerate(clust_sse):\n",
        "    print(f\"Cluster {i}: {sse}\")\n",
        "\n",
        "print(f\"\\nAverage SSE: {avg_sse}\")\n",
        "\n",
        "# Assigning labels to clusters (labels_pred)\n",
        "labels_pred = np.zeros(len(test_labels))\n",
        "for i, cluster in enumerate(clusters):\n",
        "    for sample in cluster:\n",
        "        index = np.where((test_features_pca == sample).all(axis=1))[0][0]\n",
        "        labels_pred[index] = i\n",
        "\n",
        "homogeneity, completeness, v_measure = homogeneity_completeness_v_measure(test_labels, labels_pred)\n",
        "\n",
        "print(f'Homogeneity: {homogeneity}')\n",
        "print(f'Completeness: {completeness}')\n",
        "print(f'V-measure: {v_measure}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ4Tu-C5PMCx"
      },
      "source": [
        "### Question: What is the best reduced dimension m from your experiment? What is the correpsonding clustering performance (homogeneity, completeness, and v-measure)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "703szeumPMCx"
      },
      "source": [
        "\n",
        "Test 1 : m=50, Homogeneity:0.51, Completeness:0.52, V-measure:0.51\n",
        "\n",
        "Test 2 : m=10, Homogeneity:0.47, Completeness:0.48, V-measure:0.48\n",
        "\n",
        "Test 3 : m=20, Homogeneity:0.51, Completeness:0.52, V-measure:0.52\n",
        "\n",
        "Test 4 : m=30, Homogeneity:0.49, Completeness:0.50, V-measure:0.49\n",
        "\n",
        "Test 5 : m=40, Homogeneity:0.50, Completeness:0.51, V-measure:0.50\n",
        "\n",
        "Test 6 : m=100, Homogeneity:0.51, Completeness:0.53, V-measure:0.52\n",
        "\n",
        "Test 7 : m=150, Homogeneity:0.51, Completeness:0.51, V-measure:0.51\n",
        "\n",
        "Best reduced dimension is m=100, clustering performance is:\n",
        "\n",
        "Homogeneity: 0.5146\n",
        "\n",
        "Completeness: 0.5377\n",
        "\n",
        "V-measure: 0.5262\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}